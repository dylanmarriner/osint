# OSINT Investigation Framework Specification
## Privacy/Security Risk Assessment & Identity Protection

---

## 1. Input Model (PII-Minimized)

### 1.1 Seed Identifier Schema

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "OSINT Investigation Input",
  "type": "object",
  "properties": {
    "investigation_id": {
      "type": "string",
      "format": "uuid",
      "description": "Unique identifier for this investigation"
    },
    "subject_identifiers": {
      "type": "object",
      "properties": {
        "full_name": {
          "type": "string",
          "minLength": 2,
          "maxLength": 100,
          "confidence": 0.9,
          "description": "Full legal name or commonly used name"
        },
        "known_usernames": {
          "type": "array",
          "items": {
            "type": "string",
            "minLength": 3,
            "maxLength": 30
          },
          "maxItems": 20,
          "description": "Known online usernames/handles"
        },
        "email_addresses": {
          "type": "array",
          "items": {
            "type": "string",
            "format": "email"
          },
          "maxItems": 10,
          "description": "Email addresses owned by subject"
        },
        "phone_numbers": {
          "type": "array",
          "items": {
            "type": "string",
            "pattern": "^[+]?[1-9]\\d{1,14}$"
          },
          "maxItems": 5,
          "description": "Phone numbers in E.164 format"
        },
        "geographic_hints": {
          "type": "object",
          "properties": {
            "city": { "type": "string", "maxLength": 50 },
            "region": { "type": "string", "maxLength": 50 },
            "country": { "type": "string", "maxLength": 2, "pattern": "^[A-Z]{2}$" }
          },
          "additionalProperties": false
        },
        "professional_hints": {
          "type": "object",
          "properties": {
            "employer": { "type": "string", "maxLength": 100 },
            "industry": { "type": "string", "maxLength": 50 },
            "job_title": { "type": "string", "maxLength": 100 }
          },
          "additionalProperties": false
        },
        "known_domains": {
          "type": "array",
          "items": {
            "type": "string",
            "format": "hostname"
          },
          "maxItems": 10,
          "description": "Domains associated with subject"
        }
      },
      "required": ["full_name"],
      "additionalProperties": false
    },
    "investigation_constraints": {
      "type": "object",
      "properties": {
        "exclude_sensitive_attributes": {
          "type": "boolean",
          "default": true,
          "description": "Exclude medical, financial, religious data"
        },
        "exclude_minors": {
          "type": "boolean",
          "default": true,
          "description": "Exclude data about individuals under 18"
        },
        "max_search_depth": {
          "type": "integer",
          "minimum": 1,
          "maximum": 10,
          "default": 5
        },
        "retention_days": {
          "type": "integer",
          "minimum": 1,
          "maximum": 365,
          "default": 30
        }
      },
      "additionalProperties": false
    },
    "confidence_thresholds": {
      "type": "object",
      "properties": {
        "minimum_entity_confidence": {
          "type": "number",
          "minimum": 0,
          "maximum": 100,
          "default": 70
        },
        "minimum_source_confidence": {
          "type": "number",
          "minimum": 0,
          "maximum": 100,
          "default": 60
        }
      },
      "additionalProperties": false
    }
  },
  "required": ["investigation_id", "subject_identifiers"],
  "additionalProperties": false
}
```

### 1.2 Confidence Scoring Fields

- **Source Confidence**: Reliability of data source (0-100)
- **Attribute Confidence**: Confidence in specific data point (0-100)
- **Entity Confidence**: Overall confidence in entity match (0-100)
- **Verification Status**: `verified`, `probable`, `possible`, `unlikely`

---

## 2. Collection Strategy (Public-Only)

### 2.1 High-Signal Sources Enumeration

#### 2.1.1 Search Engine Intelligence
- **Google Dorks**: 
  - `site:linkedin.com "{name}" "{company}"`
  - `site:github.com "{username}"`
  - `"{email}" filetype:pdf`
  - `"{phone}" site:facebook.com OR site:twitter.com`
- **DuckDuckGo**: Privacy-focused searches
- **Bing**: Microsoft ecosystem integration

#### 2.1.2 Social Media Platforms
- **Professional Networks**: LinkedIn, Xing
- **General Social**: Twitter/X, Facebook, Instagram
- **Niche Networks**: Reddit, Discord, Telegram (public channels)
- **Developer Platforms**: GitHub, GitLab, Bitbucket

#### 2.1.3 Public Forums & Communities
- **Technical**: Stack Overflow, Dev.to, Hacker News
- **General**: Quora, Medium comments
- **Specialized**: Industry-specific forums

#### 2.1.4 Business Directories
- **Corporate**: Company websites, press releases
- **Professional**: Industry associations, conference speaker lists
- **Academic**: University directories, publication databases

#### 2.1.5 Code Repositories
- **Public Repos**: GitHub, GitLab, SourceForge
- **Code Snippets**: Gist, Pastebin, Codepen
- **Package Registries**: npm, PyPI, Maven Central

#### 2.1.6 Breach Notification Checks
- **Public Breaches**: HaveIBeenPwned, breach databases
- **Password Leaks**: Public credential dumps
- **Email Exposure**: Public breach notifications

#### 2.1.7 Domain & Infrastructure
- **WHOIS/RDAP**: Public domain registration data
- **Certificate Transparency**: crt.sh, certificate logs
- **DNS Records**: Public DNS information
- **IP Geolocation**: Public IP intelligence

#### 2.1.8 Media & News
- **News Archives**: Google News, media databases
- **Press Releases**: Company announcements
- **Blog Mentions**: Public blog posts
- **Podcast Appearances**: Show notes, transcripts

### 2.2 Data Collection Matrix

| Source Type | Data Points Collected | Access Method | Rate Limits |
|-------------|----------------------|---------------|-------------|
| Search Engines | URLs, snippets, metadata | API/Web scraping | 100 req/hr |
| Social Profiles | Profile data, posts, connections | API/Web scraping | 200 req/hr |
| Code Repos | Commits, issues, profile info | API | 5000 req/hr |
| WHOIS | Registration data, nameservers | RDAP API | 1000 req/hr |
| Cert Logs | Domain certificates, subdomains | CT logs | Unlimited |
| Forums | Posts, signatures, profile data | Web scraping | 300 req/hr |

---

## 3. Technical Architecture

### 3.1 Modular Pipeline Design

```
Seed Input â†’ Discovery Engine â†’ Fetch Workers â†’ Parse Engine â†’ 
Normalize â†’ Enrichment â†’ Entity Resolution â†’ Scoring â†’ Report Generator
```

#### 3.1.1 Pipeline Stages

**Discovery Engine**
- Input validation and sanitization
- Query generation for each source type
- Search strategy optimization
- Duplicate query detection

**Fetch Workers**
- Distributed HTTP clients with rotation
- Rate limiting per source
- Automatic retry with exponential backoff
- Response caching (Redis)

**Parse Engine**
- Source-specific parsers
- HTML/JSON/XML extraction
- Data validation and cleaning
- Error handling and logging

**Normalization Layer**
- Standardized entity schemas
- Data type conversion
- Geographic normalization
- Temporal standardization

**Enrichment Services**
- Geolocation enrichment
- Email domain validation
- Social media platform detection
- Professional title classification

**Entity Resolution**
- Fuzzy matching algorithms
- Graph-based relationship mapping
- Confidence scoring
- Conflict resolution

**Scoring Engine**
- Multi-factor confidence calculation
- Risk assessment scoring
- Exposure category classification
- Remediation priority ranking

### 3.2 Plugin System Architecture

#### 3.2.1 Connector Interface

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime

@dataclass
class SearchResult:
    url: str
    title: str
    content: str
    metadata: Dict[str, Any]
    confidence: float
    source_type: str
    retrieved_at: datetime

class SourceConnector(ABC):
    """Base interface for all OSINT source connectors"""
    
    @property
    @abstractmethod
    def source_name(self) -> str:
        """Human-readable source name"""
        pass
    
    @property
    @abstractmethod
    def rate_limit(self) -> int:
        """Requests per hour limit"""
        pass
    
    @abstractmethod
    async def search(self, query: str, params: Dict[str, Any]) -> List[SearchResult]:
        """Execute search against source"""
        pass
    
    @abstractmethod
    async def validate_credentials(self) -> bool:
        """Validate API credentials if required"""
        pass
    
    @abstractmethod
    def get_confidence_weight(self) -> float:
        """Base confidence weight for this source"""
        pass
```

#### 3.2.2 Plugin Registration

```python
class ConnectorRegistry:
    def __init__(self):
        self._connectors: Dict[str, SourceConnector] = {}
    
    def register(self, connector: SourceConnector):
        self._connectors[connector.source_name] = connector
    
    def get_connector(self, source_name: str) -> Optional[SourceConnector]:
        return self._connectors.get(source_name)
    
    def list_connectors(self) -> List[str]:
        return list(self._connectors.keys())
```

### 3.3 Data Storage Model

#### 3.3.1 Raw Data Storage
```sql
CREATE TABLE raw_snapshots (
    id UUID PRIMARY KEY,
    investigation_id UUID NOT NULL,
    source_url TEXT NOT NULL,
    source_type VARCHAR(50) NOT NULL,
    raw_content TEXT,
    metadata JSONB,
    retrieved_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    content_hash VARCHAR(64) UNIQUE,
    encrypted_at_rest BOOLEAN DEFAULT TRUE
);

CREATE INDEX idx_raw_snapshots_investigation ON raw_snapshots(investigation_id);
CREATE INDEX idx_raw_snapshots_source_type ON raw_snapshots(source_type);
```

#### 3.3.2 Normalized Entities
```sql
CREATE TABLE entities (
    id UUID PRIMARY KEY,
    investigation_id UUID NOT NULL,
    entity_type VARCHAR(50) NOT NULL, -- person, company, domain, etc.
    attributes JSONB NOT NULL,
    confidence_score INTEGER CHECK (confidence_score >= 0 AND confidence_score <= 100),
    verification_status VARCHAR(20) DEFAULT 'possible',
    sources JSONB NOT NULL, -- Array of source references
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_entities_investigation ON entities(investigation_id);
CREATE INDEX idx_entities_type ON entities(entity_type);
CREATE INDEX idx_entities_confidence ON entities(confidence_score);
```

### 3.4 Provenance Tracking

Every extracted fact stores:
- **source_url**: Original URL
- **retrieval_time**: Timestamp of collection
- **extraction_method**: Parser/plugin used
- **content_hash**: Hash of source content
- **processing_chain**: Sequence of transformations applied

---

## 4. Entity Resolution & Verification

### 4.1 False Positive Prevention

#### 4.1.1 Evidence Thresholding
- **Strong Evidence**: Direct name + unique identifier match
- **Moderate Evidence**: Name + contextual attributes
- **Weak Evidence**: Partial name or single attribute match

#### 4.1.2 Multi-Signal Matching
```python
def calculate_match_confidence(entity_data: Dict, seed_data: Dict) -> float:
    scores = []
    
    # Name similarity (weighted: 30%)
    name_score = name_similarity(entity_data.get('name'), seed_data.get('full_name'))
    scores.append(('name', name_score, 0.3))
    
    # Username similarity (weighted: 25%)
    username_score = username_similarity(entity_data.get('usernames'), seed_data.get('known_usernames'))
    scores.append(('username', username_score, 0.25))
    
    # Geographic consistency (weighted: 20%)
    geo_score = geographic_consistency(entity_data.get('location'), seed_data.get('geographic_hints'))
    scores.append('geo', geo_score, 0.2)
    
    # Professional consistency (weighted: 15%)
    prof_score = professional_consistency(entity_data.get('work'), seed_data.get('professional_hints'))
    scores.append(('professional', prof_score, 0.15))
    
    # Temporal consistency (weighted: 10%)
    temporal_score = temporal_consistency(entity_data.get('timestamps'), seed_data.get('timeframe'))
    scores.append(('temporal', temporal_score, 0.1))
    
    # Calculate weighted average
    total_score = sum(score * weight for _, score, weight in scores)
    return min(total_score, 100.0)
```

#### 4.1.3 Geotemporal Consistency
- **Geographic**: Location proximity within reasonable distance
- **Temporal**: Activity timestamps within plausible ranges
- **Cross-Platform**: Consistent timelines across platforms

### 4.2 Confidence Scoring Rubric

| Score Range | Description | Evidence Required |
|-------------|-------------|-------------------|
| 90-100 | Verified Match | Strong name match + 2+ unique identifiers |
| 75-89 | Probable Match | Name match + 1+ contextual attributes |
| 60-74 | Possible Match | Partial name or single strong attribute |
| 40-59 | Unlikely Match | Weak similarity, high collision risk |
| 0-39 | No Match | Insufficient evidence |

### 4.3 Human-in-the-Loop Checkpoints

- **Ambiguous Matches**: Confidence 60-74 requires manual review
- **High-Risk Findings**: Sensitive data requires verification
- **Cross-Platform Conflicts**: Inconsistent data across sources
- **Legal/Privacy Flags**: Potentially restricted data

---

## 5. Output & Reporting

### 5.1 Structured Report Schema

```json
{
  "report_metadata": {
    "investigation_id": "uuid",
    "generated_at": "2024-01-15T10:30:00Z",
    "report_version": "1.0",
    "data_retention_days": 30
  },
  "executive_summary": {
    "risk_level": "HIGH|MEDIUM|LOW",
    "total_findings": 42,
    "high_risk_findings": 8,
    "key_exposures": ["email_address", "phone_number", "home_address"],
    "recommendation_priority": "IMMEDIATE"
  },
  "identity_inventory": {
    "verified_identities": [
      {
        "platform": "LinkedIn",
        "username": "john-doe",
        "confidence": 95,
        "verification_status": "verified",
        "profile_url": "https://linkedin.com/in/john-doe"
      }
    ],
    "probable_identities": [...],
    "possible_identities": [...]
  },
  "exposure_analysis": {
    "contact_information": {
      "email_addresses": {
        "exposed_count": 3,
        "risk_level": "HIGH",
        "sources": ["public_profile", "breach_data"]
      },
      "phone_numbers": {
        "exposed_count": 1,
        "risk_level": "MEDIUM",
        "sources": ["public_directory"]
      }
    },
    "account_takeover_risk": {
      "compromised_passwords": 2,
      "reused_passwords": 1,
      "security_questions": 0,
      "risk_score": 75
    },
    "doxxing_risk": {
      "address_exposure": false,
      "family_connections": 3,
      "employment_history": 2,
      "risk_score": 60
    },
    "impersonation_risk": {
      "profile_completeness": 85,
      "verification_badges": 1,
      "public_followers": 500,
      "risk_score": 45
    }
  },
  "activity_timeline": [
    {
      "date": "2024-01-10",
      "platform": "GitHub",
      "activity": "Code commit to public repository",
      "privacy_implication": "LOW"
    }
  ],
  "remediation_recommendations": [
    {
      "priority": "HIGH",
      "category": "account_security",
      "action": "Enable two-factor authentication",
      "platforms": ["email", "social_media"],
      "estimated_impact": "85%",
      "implementation_difficulty": "LOW"
    }
  ],
  "detailed_findings": [...],
  "source_references": [...]
}
```

### 5.2 Human-Readable Format

# OSINT Privacy Risk Assessment Report

## Executive Summary
**Risk Level**: ðŸ”´ HIGH  
**Total Findings**: 42  
**Critical Actions Required**: 8  

### Key Exposures Identified
- **Email Addresses**: 3 addresses exposed in public profiles
- **Phone Number**: 1 number found in public directory
- **Professional History**: Complete employment timeline visible

### Immediate Recommendations
1. **Enable 2FA** on all identified accounts (Impact: 85%)
2. **Remove personal phone** from public directories
3. **Audit social media privacy** settings

---

## 6. Safety, Privacy, and Abuse Prevention

### 6.1 Data Minimization & Retention

#### 6.1.1 Retention Policy
- **Default Retention**: 30 days
- **Extended Retention**: 90 days (with explicit consent)
- **Automatic Deletion**: Scheduled cleanup jobs
- **Manual Purge**: Immediate deletion on request

#### 6.1.2 Data Redaction
```python
def redact_sensitive_data(text: str) -> str:
    """Redact sensitive patterns from text"""
    patterns = {
        'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        'phone': r'\b(?:\+?1[-.\s]?)?\(?[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4}\b',
        'ssn': r'\b\d{3}-\d{2}-\d{4}\b',
        'credit_card': r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b'
    }
    
    redacted = text
    for pattern_type, pattern in patterns.items():
        redacted = re.sub(pattern, f'[REDACTED_{pattern_type.upper()}]', redacted)
    
    return redacted
```

#### 6.1.3 Hash-Based Storage
- Store SHA-256 hashes of sensitive identifiers
- Use salted hashes for lookup operations
- Maintain reverse mapping only for active investigations

### 6.2 Opt-Out Mechanism

#### 6.2.1 Subject Access Request Process
1. **Verification**: Identity confirmation required
2. **Data Export**: Complete data package provided
3. **Deletion Request**: Immediate processing
4. **Confirmation**: Deletion verification sent

#### 6.2.2 Do-Not-Track Registry
- Central opt-out database
- Automatic exclusion from future investigations
- Regular audit of compliance

### 6.3 Abuse Prevention

#### 6.3.1 Usage Monitoring
- Investigation frequency limits per user
- Suspicious pattern detection
- Automated abuse flagging

#### 6.3.2 Access Controls
- Role-based permissions
- Investigation purpose validation
- Audit logging for all access

---

## 7. Implementation Blueprint

### 7.1 Folder Structure

```
osint-framework/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”‚   â”œâ”€â”€ discovery.py
â”‚   â”‚   â”‚   â”œâ”€â”€ fetch.py
â”‚   â”‚   â”‚   â”œâ”€â”€ parse.py
â”‚   â”‚   â”‚   â”œâ”€â”€ normalize.py
â”‚   â”‚   â”‚   â”œâ”€â”€ enrich.py
â”‚   â”‚   â”‚   â”œâ”€â”€ resolve.py
â”‚   â”‚   â”‚   â””â”€â”€ score.py
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ entities.py
â”‚   â”‚   â”‚   â”œâ”€â”€ investigation.py
â”‚   â”‚   â”‚   â””â”€â”€ reports.py
â”‚   â”‚   â””â”€â”€ storage/
â”‚   â”‚       â”œâ”€â”€ database.py
â”‚   â”‚       â””â”€â”€ cache.py
â”‚   â”œâ”€â”€ connectors/
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ search_engines.py
â”‚   â”‚   â”œâ”€â”€ social_media.py
â”‚   â”‚   â”œâ”€â”€ code_repos.py
â”‚   â”‚   â””â”€â”€ directories.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ validation.py
â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”‚   â””â”€â”€ monitoring.py
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ endpoints.py
â”‚       â””â”€â”€ middleware.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ sources.yaml
â”‚   â”œâ”€â”€ rate_limits.yaml
â”‚   â””â”€â”€ scoring_weights.yaml
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ api_reference.md
â”‚   â”œâ”€â”€ connector_development.md
â”‚   â””â”€â”€ security_guidelines.md
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.py
â”‚   â”œâ”€â”€ migrate.py
â”‚   â””â”€â”€ cleanup.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

### 7.2 Example Connector Implementation

```python
# connectors/social_media.py
from .base import SourceConnector, SearchResult
import aiohttp
import asyncio
from typing import Dict, List, Any

class LinkedInConnector(SourceConnector):
    @property
    def source_name(self) -> str:
        return "linkedin"
    
    @property
    def rate_limit(self) -> int:
        return 100  # requests per hour
    
    async def search(self, query: str, params: Dict[str, Any]) -> List[SearchResult]:
        """Search LinkedIn profiles"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; OSINT-Framework/1.0)',
            'Accept-Language': 'en-US,en;q=0.9'
        }
        
        search_url = f"https://www.linkedin.com/search/results/people/?keywords={query}"
        
        async with aiohttp.ClientSession() as session:
            async with session.get(search_url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    return self._parse_profiles(html, query)
                else:
                    return []
    
    def _parse_profiles(self, html: str, query: str) -> List[SearchResult]:
        """Parse LinkedIn search results"""
        # Implementation would use BeautifulSoup or similar
        # This is a simplified example
        results = []
        
        # Extract profile data from HTML
        # ... parsing logic ...
        
        return results
    
    async def validate_credentials(self) -> bool:
        """LinkedIn doesn't require API keys for public search"""
        return True
    
    def get_confidence_weight(self) -> float:
        return 0.85  # LinkedIn has high reliability
```

### 7.3 Pipeline Pseudocode

```python
async def run_investigation(input_data: Dict) -> Dict:
    """Main investigation pipeline"""
    
    # Initialize pipeline components
    discovery = DiscoveryEngine()
    fetcher = FetchManager()
    parser = ParseEngine()
    normalizer = NormalizationEngine()
    resolver = EntityResolver()
    scorer = ScoringEngine()
    reporter = ReportGenerator()
    
    # Stage 1: Discovery
    search_queries = discovery.generate_queries(input_data)
    
    # Stage 2: Fetch (parallel execution)
    fetch_tasks = []
    for connector, queries in search_queries.items():
        for query in queries:
            task = fetcher.fetch(connector, query)
            fetch_tasks.append(task)
    
    raw_results = await asyncio.gather(*fetch_tasks)
    
    # Stage 3: Parse
    parsed_data = []
    for result in raw_results:
        parsed = parser.parse(result)
        parsed_data.extend(parsed)
    
    # Stage 4: Normalize
    normalized_entities = normalizer.normalize(parsed_data)
    
    # Stage 5: Entity Resolution
    resolved_entities = resolver.resolve(normalized_entities, input_data)
    
    # Stage 6: Scoring
    scored_entities = scorer.score_entities(resolved_entities)
    
    # Stage 7: Generate Report
    report = reporter.generate_report(scored_entities, input_data)
    
    return report
```

### 7.4 Test Strategy

#### 7.4.1 Unit Tests
```python
# tests/unit/test_entity_resolution.py
import pytest
from src.core.pipeline.resolve import EntityResolver

class TestEntityResolution:
    def test_name_matching_exact(self):
        resolver = EntityResolver()
        entity = {"name": "John Doe", "platform": "LinkedIn"}
        seed = {"full_name": "John Doe"}
        
        confidence = resolver.calculate_name_similarity(entity, seed)
        assert confidence >= 90
    
    def test_name_matching_partial(self):
        resolver = EntityResolver()
        entity = {"name": "John M. Doe", "platform": "Twitter"}
        seed = {"full_name": "John Doe"}
        
        confidence = resolver.calculate_name_similarity(entity, seed)
        assert 60 <= confidence < 90
```

#### 7.4.2 Integration Tests
```python
# tests/integration/test_full_pipeline.py
import pytest
from src.core.pipeline import run_investigation

@pytest.mark.asyncio
async def test_full_pipeline():
    input_data = {
        "investigation_id": "test-123",
        "subject_identifiers": {
            "full_name": "Jane Smith",
            "known_usernames": ["janesmith"]
        }
    }
    
    result = await run_investigation(input_data)
    
    assert "executive_summary" in result
    assert "identity_inventory" in result
    assert isinstance(result["executive_summary"]["total_findings"], int)
```

### 7.5 Monitoring Metrics

#### 7.5.1 Operational Metrics
- **Pipeline Success Rate**: Percentage of completed investigations
- **Source Availability**: Uptime per data source
- **Average Processing Time**: Time per investigation
- **Error Rates**: Failed requests per source

#### 7.5.2 Security Metrics
- **Access Attempts**: Login/authorization attempts
- **Data Access Logs**: Investigation data access patterns
- **Abuse Flags**: Suspicious activity detection
- **Retention Compliance**: Data deletion compliance

### 7.6 Operational Runbooks

#### 7.6.1 Incident Response
1. **Data Breach Detection**: Immediate isolation
2. **Impact Assessment**: Scope determination
3. **Notification**: Legal/compliance teams
4. **Remediation**: System hardening
5. **Post-Mortem**: Process improvement

#### 7.6.2 System Maintenance
- **Daily**: Health checks, log review
- **Weekly**: Performance optimization, security updates
- **Monthly**: Data cleanup, connector updates
- **Quarterly**: Security audit, compliance review

---

## 8. Optional Enhancers

### 8.1 Privacy Exposure Scoring Model

#### 8.1.1 Scoring Categories & Weights

| Category | Weight | Factors |
|----------|--------|---------|
| Contact Information | 30% | Email, phone, address exposure |
| Professional Data | 25% | Employment history, skills, education |
| Personal Identity | 20% | Photos, biographical data |
| Behavioral Data | 15% | Posts, likes, comments |
| Network Connections | 10% | Friends, colleagues, family |

#### 8.1.2 Exposure Score Calculation
```python
def calculate_exposure_score(exposures: Dict) -> Dict:
    category_scores = {}
    
    # Contact Information (30% weight)
    contact_score = min(100, 
        len(exposures.get('emails', [])) * 20 +
        len(exposures.get('phones', [])) * 25 +
        len(exposures.get('addresses', [])) * 30
    )
    category_scores['contact'] = contact_score * 0.3
    
    # Professional Data (25% weight)
    professional_score = min(100,
        len(exposures.get('employers', [])) * 15 +
        len(exposures.get('positions', [])) * 10 +
        len(exposures.get('education', [])) * 12
    )
    category_scores['professional'] = professional_score * 0.25
    
    # ... other categories
    
    total_score = sum(category_scores.values())
    
    return {
        'total_score': total_score,
        'category_breakdown': category_scores,
        'risk_level': 'HIGH' if total_score > 70 else 'MEDIUM' if total_score > 40 else 'LOW'
    }
```

#### 8.1.3 Remediation Impact Estimates

| Action | Estimated Impact | Implementation Difficulty |
|--------|------------------|---------------------------|
| Enable 2FA | 85% | LOW |
| Remove phone from public | 70% | MEDIUM |
| Set social profiles to private | 60% | MEDIUM |
| Delete unused accounts | 45% | HIGH |

### 8.2 Threat Model (STRIDE Framework)

#### 8.2.1 Threat Analysis

| Threat | Description | Mitigation |
|--------|-------------|------------|
| **Spoofing** | Fake investigation requests | Authentication, request validation |
| **Tampering** | Data modification during collection | Encryption, integrity checks |
| **Repudiation** | Denial of investigation activities | Comprehensive audit logging |
| **Information Disclosure** | Unauthorized data access | Access controls, encryption |
| **Denial of Service** | System overload | Rate limiting, resource quotas |
| **Elevation of Privilege** | Admin access abuse | Principle of least privilege |

#### 8.2.2 Security Controls

- **Authentication**: Multi-factor auth for all access
- **Authorization**: Role-based access control
- **Encryption**: AES-256 at rest, TLS 1.3 in transit
- **Audit Logging**: Immutable audit trail
- **Network Security**: VPN, firewall rules
- **Monitoring**: Real-time threat detection

### 8.3 Sample End-to-End Run

#### 8.3.1 Input Data
```json
{
  "investigation_id": "sample-001",
  "subject_identifiers": {
    "full_name": "Alex Johnson",
    "known_usernames": ["alexj", "ajohnson"],
    "email_addresses": ["alex.johnson@techcorp.com"],
    "geographic_hints": {
      "city": "San Francisco",
      "region": "California"
    },
    "professional_hints": {
      "employer": "TechCorp",
      "industry": "Software Engineering"
    }
  }
}
```

#### 8.3.2 Intermediate Entities
```json
{
  "discovered_entities": [
    {
      "platform": "LinkedIn",
      "username": "alexjohnson",
      "confidence": 92,
      "attributes": {
        "name": "Alex Johnson",
        "title": "Senior Software Engineer",
        "company": "TechCorp",
        "location": "San Francisco Bay Area"
      }
    },
    {
      "platform": "GitHub",
      "username": "alexj",
      "confidence": 85,
      "attributes": {
        "name": "Alex Johnson",
        "email": "alex.johnson@techcorp.com",
        "location": "San Francisco"
      }
    }
  ]
}
```

#### 8.3.3 Final Report Output
```json
{
  "executive_summary": {
    "risk_level": "MEDIUM",
    "total_findings": 12,
    "high_risk_findings": 2,
    "key_exposures": ["professional_email", "employment_history"],
    "recommendation_priority": "MODERATE"
  },
  "identity_inventory": {
    "verified_identities": 2,
    "probable_identities": 1,
    "possible_identities": 3
  },
  "exposure_analysis": {
    "contact_information": {
      "exposed_count": 1,
      "risk_level": "MEDIUM"
    },
    "professional_data": {
      "exposure_level": "HIGH",
      "risk_score": 78
    }
  }
}
```

### 8.4 MVP Roadmap

#### 8.4.1 Minimal Viable Product (MVP)
**Timeline**: 8-10 weeks
**Features**:
- Basic input validation
- 3 core connectors (Google, LinkedIn, GitHub)
- Simple entity resolution
- Basic report generation
- 30-day data retention

#### 8.4.2 Version 1.0
**Timeline**: 4-6 months post-MVP
**Features**:
- Full connector suite (10+ sources)
- Advanced entity resolution
- Comprehensive reporting
- User management system
- API endpoints

#### 8.4.3 Version 2.0
**Timeline**: 8-12 months post-v1
**Features**:
- Machine learning enrichment
- Real-time monitoring
- Advanced analytics
- Multi-language support
- Enterprise features

### 8.5 Jurisdiction-Aware Compliance Notes

#### 8.5.1 New Zealand
- **Privacy Act 2020**: Requires reasonable security safeguards
- **Harassment Act**: Consideration for doxxing prevention
- **Human Rights Act**: Discrimination prevention in data use

#### 8.5.2 Australia
- **Privacy Act 1988**: Australian Privacy Principles (APPs)
- **Notifiable Data Breaches Scheme**: Mandatory breach reporting
- **Online Safety Act**: Harassment and abuse prevention

#### 8.5.3 United States
- **CCPA/CPRA (California)**: Consumer privacy rights
- **GDPR considerations**: For EU subjects
- **State-specific laws**: Varying privacy requirements

#### 8.5.4 European Union
- **GDPR**: Strict data protection requirements
- **Right to be Forgotten**: Mandatory deletion requests
- **Data Processing Agreements**: Required for processors

#### 8.5.5 Compliance Framework
```python
class ComplianceEngine:
    def __init__(self, jurisdiction: str):
        self.jurisdiction = jurisdiction
        self.rules = self.load_compliance_rules(jurisdiction)
    
    def validate_processing(self, investigation_data: Dict) -> bool:
        """Check if investigation complies with jurisdiction rules"""
        for rule in self.rules:
            if not rule.validate(investigation_data):
                return False
        return True
    
    def get_retention_period(self) -> int:
        """Return jurisdiction-specific retention period"""
        retention_map = {
            'NZ': 30,
            'AU': 45,
            'US': 90,
            'EU': 30
        }
        return retention_map.get(self.jurisdiction, 30)
```

---

## Conclusion

This OSINT investigation framework provides a comprehensive, privacy-focused approach to digital risk assessment. By combining modular architecture, robust entity resolution, and strong privacy safeguards, it enables effective security assessments while maintaining ethical standards and regulatory compliance.

The framework is designed to be:
- **Scalable**: Modular plugin architecture supports growth
- **Secure**: Multiple layers of security and privacy protection
- **Compliant**: Jurisdiction-aware and regulation-ready
- **Ethical**: Built-in safeguards against abuse
- **Maintainable**: Clear separation of concerns and comprehensive testing

Implementation should follow the phased roadmap, starting with the MVP to validate core functionality before expanding to full enterprise capabilities.
